import os
import random
from typing import List
from dataclasses import dataclass, field
from functools import lru_cache
import requests
import wikipediaapi
import praw
from dotenv import load_dotenv
from ffmpeg import FFmpeg
from elevenlabs import generate, save
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings
from langchain.vectorstores.redis import RedisVectorStoreRetriever, Redis
from langchain.schema import Document
from huggingface_hub import InferenceClient
from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint
from langchain_community.llms.ollama import Ollama
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
import google.generativeai as genai



load_dotenv("./.env")

CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
GOOGLE_AI_API_TOKEN = os.getenv("GOOGLE_AI_API_TOKEN")
REDPO_ID = "mistralai/Mistral-7B-Instruct-v0.2"
ENDPOINT_URL = f"https://api-inference.huggingface.co/models/{REDPO_ID}"
NER_REPO_ID = "jean-baptiste/roberta-large-ner-english"
WIKI_API_SEARCH_URL = "https://en.wikipedia.org/w/rest.php/v1/search/page?q={}&limit=4"
REDIS_PORT = os.getenv("REDIS_PORT")
REDIS_HOST = os.getenv("REDIS_HOST")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
TEXT_TO_QUESTION_MODEL = os.getenv("TEXT_TO_QUESTION_MODEL")

redis_url = f"redis://{REDIS_HOST}:{REDIS_PORT}"

# embedding_model = SentenceTransformer("path/to/model")


wiki_wiki = wikipediaapi.Wikipedia("MyProjectName (merlin@example.com)", "en")
ner_model = InferenceClient(token=HUGGINGFACEHUB_API_TOKEN)
embeddings = HuggingFaceBgeEmbeddings(model_name=EMBEDDING_MODEL)


class StoryPrompt(BaseModel):
    model_output: str = Field(description="the output generated by the model")


@dataclass
class Media:
    location: str
    name: str = field(init=False)
    file_format: str = field(init=False)
    size: int = field(init=False)
    duration: int = field(init=False)

    def __post__init__(self):
        pass

    def get_file_info(self):
        pass


@dataclass
class WikiPage:
    page_title: str
    text: str

    def __post_init__(self):
        self.page_title = self.page_title.lower()

    def __str__(self):
        return self.page_title


@dataclass
class Audio(Media):
    pass


@dataclass
class Video(Media):
    pass


def get_videos_from_subreddit():
    reddit = praw.Reddit(
        client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent="vidoe_bot"
    )

    # Get the subreddit instance
    subreddit = reddit.subreddit("oddlysatisfying")

    # Get video submissions
    videos = []
    for submission in subreddit.hot(limit=40):
        if (
            submission.secure_media is not None
            and submission.secure_media.get("reddit_video") is not None
        ):
            video_data = submission.secure_media["reddit_video"]
            duration = video_data.get("duration", 0)
            height = video_data.get("height", 0)
            width = video_data.get("width", 0)
            scrubber_media_url = video_data.get("fallback_url")

            if (
                20 <= duration <= 60
                and height >= 1000
                and width >= 1000
                and scrubber_media_url
            ):
                videos.append(
                    {
                        "title": submission.title,
                        "url": scrubber_media_url,
                        "author": submission.author.name,
                    }
                )
    return random.choice(videos)


def convert_video(video_file: str, audio_file: str):

    ffmpeg = (
        FFmpeg()
        .option("y")
        .input(video_file, stream_loop=-1)
        .input(audio_file)
        .output(
            "output.mp4",
            options={"codec:a": "libmp3lame", "filter:v": "scale=-1:1080"},
            map=["0:v:0", "1:a:0"],
            shortest=None,
        )
    )

    ffmpeg.execute()


def convert_text_to_audio(text: str):
    audio = generate(api_key=ELEVENLABS_API_KEY, text=text, voice="Adam")
    save(audio=audio, filename="output.wav")
    return None


def open_prompt_txt() -> str:
    """
    Reads and returns the content of the file 'prompt.txt' as a string.
    """
    with open("prompt.txt", "r", encoding="utf-8") as f:
        return f.read()

def extract_json(llm_output):
    """
    Extracts JSON data from a given string.

    Parameters:
        llm_output (str): The string containing the JSON data.

    Returns:
        str: The extracted JSON string.
    """
    # Find the start and end index of JSON data
    start_index = llm_output.find("{")
    end_index = llm_output.rfind("}") + 1

    # Extract JSON string
    json_str = llm_output[start_index:end_index]
    return json_str
    # Load JSON string into a Python dictionary
    # json_data = json.loads(json_str)

    # return json_data

def get_story(user_prompt: str, context_documents: List[Document]):
    """
    Generates a story question based on the user prompt and context documents.

    Parameters:
        user_prompt (str): The user prompt for generating the story question.
        context_documents (List[Document]): A list of documents providing context for the story.

    Returns:
        str: The generated story question.
    """

    parser = JsonOutputParser(pydantic_object=StoryPrompt)

    llm = HuggingFaceEndpoint(
        endpoint_url=ENDPOINT_URL,
        temperature=0.2,
        repetition_penalty=1.2,
        cache=False,
        top_k=20,
        top_p=0.95,
        max_new_tokens=200,
    )
    prompt_template = open_prompt_txt()
    final_prompt = PromptTemplate(
        input_variables=["documents", "user"],
        template=prompt_template,
        # partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    chain = LLMChain(llm=llm, prompt=final_prompt)
    question = chain.invoke({"user": user_prompt, "documents": context_documents})
    print(question["text"])
    # print("done")
    # return extract_json(question["text"])


def return_ner_tokens(text: str):
    """
    A function that returns named entity recognition (NER) tokens from the given text.

    Parameters:
    - text (str): The input text for which NER tokens need to be extracted.

    Returns:
    - list: A list of NER tokens extracted from the input text.
    """
    result = ner_model.token_classification(text=text, model=NER_REPO_ID)
    return [i["word"].strip() for i in result]


def wiki_search(query: str):
    """
    Searches for a given query on the Wikipedia API and returns a list of page keys.

    Parameters:
        query (str): The search query to be used for the Wikipedia API.

    Returns:
        list: A list of page keys retrieved from the Wikipedia API response.

    Raises:
        None

    Examples:
        >>> wiki_search("Python")
        ['/wiki/Python', '/wiki/Python_(programming_language)', '/wiki/Python_(film)']
    """
    params = {
        "action": "query",
        "format": "json",
        "prop": "revisions",
        "rvprop": "content",
        "rvslots": "main",
        "titles": query,
    }

    # Send the API request
    response = requests.get(
        WIKI_API_SEARCH_URL.format(query), params=params, timeout=60
    )

    if response.status_code == 200:
        # Parse the JSON data
        data = response.json()

        # Extract the page content from the response
        pages = data["pages"]
        return [page["key"].lower() for page in pages]

    else:
        print("Failed to retrieve page content.")


def get_page_content(title: str) -> WikiPage:
    """
    A function that retrieves the content of a Wikipedia page based on the provided title.

    Args:
        title (str): The title of the Wikipedia page to retrieve.

    Returns:
        WikiPage: An instance of WikiPage containing the title and text of the Wikipedia page.
    """
    return WikiPage(page_title=title, text=wiki_wiki.page(title=title).text)


def chunk_and_save(pages: List[WikiPage]):
    """
    Splits the text of each WikiPage in the given list into smaller chunks using the RecursiveCharacterTextSplitter.
    
    Args:
        pages (List[WikiPage]): A list of WikiPage objects containing the text to be split.
        
    Returns:
        List[Redis]: A list of Redis objects created from the split text of each WikiPage.
    """
    

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=250)
    page_splits = [
        WikiPage(page_title=page.page_title, text=text_splitter.split_text(page.text))
        for page in pages
        if page.text != ""
    ]
    result = [
        Redis.from_texts(
            page.text, embeddings, redis_url=redis_url, index_name=page.page_title
        )
        for page in page_splits
    ]


def return_documents(user_prompt: str, *, index_names: List[str]) -> List[Document]:
    """
    Generates a list of Document objects by invoking the RedisVectorStoreRetriever with the given user prompt and index names.

    Args:
        user_prompt (str): The user prompt to be passed to the RedisVectorStoreRetriever.
        index_names (List[str]): The list of index names to be used by the RedisVectorStoreRetriever.

    Returns:
        List[Document]: The list of Document objects generated by the RedisVectorStoreRetriever.
    """
    return [
        RedisVectorStoreRetriever(
            vectorstore=Redis(
                redis_url=redis_url, embedding=embeddings, index_name=index_name.lower()
            ),
            search_kwargs={"k": 20, "distance_threshold": None},
        ).invoke(user_prompt)
        for index_name in index_names
    ]


# indexs = ["air_force_one", "airforce_delta_strike","women_airforce_service_pilots","finnish_air_force"]
prompt = "write a story on how JFK was assasinated and what happned after"

# print(return_documents(prompt, index_names=indexs))

# tokens = return_ner_tokens(prompt)
# print(tokens)
# search = [wiki_search(token) for token in tokens]
# print(search)
# contents = [get_page_content(title) for titles in search for title in titles]
# # print([i for i in content])
# chunk_and_save(contents)

#
documents = return_documents(
    prompt,
    index_names=["john_f._kennedy", "assassination_of_john_f._kennedy", "jfk_(film)"],
)
# # print(documents)

get_story(user_prompt=prompt, context_documents=documents)

# print(
#     convert_user_input_to_question(
#         "a story about how the president of the unitedstates travels and the way his convoy is secured and protected by various security personnels"
#     )
# )

# print("TEXT_TO_QUESTION_MODEL: ", \TEXT_TO_QUESTION_MODEL)

# from huggingface_hub import InferenceClient
